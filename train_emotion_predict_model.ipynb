{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb0a6515-e283-4d8e-a713-e7bb0f6f58dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57440ea9-75cb-4aa4-9ec8-92fe55c1d405",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %edit /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/keras_vggface/models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f0aa2f7-a377-456e-87e6-18fc72ea4739",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# with open('/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/keras_vggface/models.py', 'r') as file:\n",
    "#     content = file.read()\n",
    "\n",
    "# new_content = content.replace('from keras.engine.topology import get_source_inputs', 'from keras.utils import get_source_inputs')\n",
    "\n",
    "# with open('/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/keras_vggface/models.py', 'w') as file:\n",
    "#     file.write(new_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6c9b3ef-9a13-4095-b952-53c06e62f0a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'''VGGFace models for Keras.\n\n\n\n# Notes:\n\n- Resnet50 and VGG16  are modified architectures from Keras Application folder. [Keras](https://keras.io)\n\n\n\n- Squeeze and excitation block is taken from  [Squeeze and Excitation Networks in\n\n Keras](https://github.com/titu1994/keras-squeeze-excite-network) and modified.\n\n\n\n'''\n\n\n\n\n\nfrom keras.layers import Flatten, Dense, Input, GlobalAveragePooling2D, \\\n\n    GlobalMaxPooling2D, Activation, Conv2D, MaxPooling2D, BatchNormalization, \\\n\n    AveragePooling2D, Reshape, Permute, multiply\n\nfrom keras_applications.imagenet_utils import _obtain_input_shape\n\nfrom keras.utils import layer_utils\n\nfrom keras.utils.data_utils import get_file\n\nfrom keras import backend as K\n\nfrom keras_vggface import utils\n\nfrom keras.utils import get_source_inputs\n\nimport warnings\n\nfrom keras.models import Model\n\nfrom keras import layers\n\n\n\n\n\ndef VGG16(include_top=True, weights='vggface',\n\n          input_tensor=None, input_shape=None,\n\n          pooling=None,\n\n          classes=2622):\n\n    input_shape = _obtain_input_shape(input_shape,\n\n                                      default_size=224,\n\n                                      min_size=48,\n\n                                      data_format=K.image_data_format(),\n\n                                      require_flatten=include_top)\n\n\n\n    if input_tensor is None:\n\n        img_input = Input(shape=input_shape)\n\n    else:\n\n        if not K.is_keras_tensor(input_tensor):\n\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n\n        else:\n\n            img_input = input_tensor\n\n\n\n    # Block 1\n\n    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='conv1_1')(\n\n        img_input)\n\n    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='conv1_2')(x)\n\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='pool1')(x)\n\n\n\n    # Block 2\n\n    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='conv2_1')(\n\n        x)\n\n    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='conv2_2')(\n\n        x)\n\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='pool2')(x)\n\n\n\n    # Block 3\n\n    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='conv3_1')(\n\n        x)\n\n    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='conv3_2')(\n\n        x)\n\n    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='conv3_3')(\n\n        x)\n\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='pool3')(x)\n\n\n\n    # Block 4\n\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv4_1')(\n\n        x)\n\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv4_2')(\n\n        x)\n\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv4_3')(\n\n        x)\n\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='pool4')(x)\n\n\n\n    # Block 5\n\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv5_1')(\n\n        x)\n\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv5_2')(\n\n        x)\n\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv5_3')(\n\n        x)\n\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='pool5')(x)\n\n\n\n    if include_top:\n\n        # Classification block\n\n        x = Flatten(name='flatten')(x)\n\n        x = Dense(4096, name='fc6')(x)\n\n        x = Activation('relu', name='fc6/relu')(x)\n\n        x = Dense(4096, name='fc7')(x)\n\n        x = Activation('relu', name='fc7/relu')(x)\n\n        x = Dense(classes, name='fc8')(x)\n\n        x = Activation('softmax', name='fc8/softmax')(x)\n\n    else:\n\n        if pooling == 'avg':\n\n            x = GlobalAveragePooling2D()(x)\n\n        elif pooling == 'max':\n\n            x = GlobalMaxPooling2D()(x)\n\n\n\n            # Ensure that the model takes into account\n\n            # any potential predecessors of `input_tensor`.\n\n    if input_tensor is not None:\n\n        inputs = get_source_inputs(input_tensor)\n\n    else:\n\n        inputs = img_input\n\n        # Create model.\n\n    model = Model(inputs, x, name='vggface_vgg16')  # load weights\n\n    if weights == 'vggface':\n\n        if include_top:\n\n            weights_path = get_file('rcmalli_vggface_tf_vgg16.h5',\n\n                                    utils.\n\n                                    VGG16_WEIGHTS_PATH,\n\n                                    cache_subdir=utils.VGGFACE_DIR)\n\n        else:\n\n            weights_path = get_file('rcmalli_vggface_tf_notop_vgg16.h5',\n\n                                    utils.VGG16_WEIGHTS_PATH_NO_TOP,\n\n                                    cache_subdir=utils.VGGFACE_DIR)\n\n        model.load_weights(weights_path, by_name=True)\n\n        if K.backend() == 'theano':\n\n            layer_utils.convert_all_kernels_in_model(model)\n\n\n\n        if K.image_data_format() == 'channels_first':\n\n            if include_top:\n\n                maxpool = model.get_layer(name='pool5')\n\n                shape = maxpool.output_shape[1:]\n\n                dense = model.get_layer(name='fc6')\n\n                layer_utils.convert_dense_weights_data_format(dense, shape,\n\n                                                              'channels_first')\n\n\n\n            if K.backend() == 'tensorflow':\n\n                warnings.warn('You are using the TensorFlow backend, yet you '\n\n                              'are using the Theano '\n\n                              'image data format convention '\n\n                              '(`image_data_format=\"channels_first\"`). '\n\n                              'For best performance, set '\n\n                              '`image_data_format=\"channels_last\"` in '\n\n                              'your Keras config '\n\n                              'at ~/.keras/keras.json.')\n\n    return model\n\n\n\n\n\ndef resnet_identity_block(input_tensor, kernel_size, filters, stage, block,\n\n                          bias=False):\n\n    filters1, filters2, filters3 = filters\n\n    if K.image_data_format() == 'channels_last':\n\n        bn_axis = 3\n\n    else:\n\n        bn_axis = 1\n\n    conv1_reduce_name = 'conv' + str(stage) + \"_\" + str(block) + \"_1x1_reduce\"\n\n    conv1_increase_name = 'conv' + str(stage) + \"_\" + str(\n\n        block) + \"_1x1_increase\"\n\n    conv3_name = 'conv' + str(stage) + \"_\" + str(block) + \"_3x3\"\n\n\n\n    x = Conv2D(filters1, (1, 1), use_bias=bias, name=conv1_reduce_name)(\n\n        input_tensor)\n\n    x = BatchNormalization(axis=bn_axis, name=conv1_reduce_name + \"/bn\")(x)\n\n    x = Activation('relu')(x)\n\n\n\n    x = Conv2D(filters2, kernel_size, use_bias=bias,\n\n               padding='same', name=conv3_name)(x)\n\n    x = BatchNormalization(axis=bn_axis, name=conv3_name + \"/bn\")(x)\n\n    x = Activation('relu')(x)\n\n\n\n    x = Conv2D(filters3, (1, 1), use_bias=bias, name=conv1_increase_name)(x)\n\n    x = BatchNormalization(axis=bn_axis, name=conv1_increase_name + \"/bn\")(x)\n\n\n\n    x = layers.add([x, input_tensor])\n\n    x = Activation('relu')(x)\n\n    return x\n\n\n\n\n\ndef resnet_conv_block(input_tensor, kernel_size, filters, stage, block,\n\n                      strides=(2, 2), bias=False):\n\n    filters1, filters2, filters3 = filters\n\n    if K.image_data_format() == 'channels_last':\n\n        bn_axis = 3\n\n    else:\n\n        bn_axis = 1\n\n    conv1_reduce_name = 'conv' + str(stage) + \"_\" + str(block) + \"_1x1_reduce\"\n\n    conv1_increase_name = 'conv' + str(stage) + \"_\" + str(\n\n        block) + \"_1x1_increase\"\n\n    conv1_proj_name = 'conv' + str(stage) + \"_\" + str(block) + \"_1x1_proj\"\n\n    conv3_name = 'conv' + str(stage) + \"_\" + str(block) + \"_3x3\"\n\n\n\n    x = Conv2D(filters1, (1, 1), strides=strides, use_bias=bias,\n\n               name=conv1_reduce_name)(input_tensor)\n\n    x = BatchNormalization(axis=bn_axis, name=conv1_reduce_name + \"/bn\")(x)\n\n    x = Activation('relu')(x)\n\n\n\n    x = Conv2D(filters2, kernel_size, padding='same', use_bias=bias,\n\n               name=conv3_name)(x)\n\n    x = BatchNormalization(axis=bn_axis, name=conv3_name + \"/bn\")(x)\n\n    x = Activation('relu')(x)\n\n\n\n    x = Conv2D(filters3, (1, 1), name=conv1_increase_name, use_bias=bias)(x)\n\n    x = BatchNormalization(axis=bn_axis, name=conv1_increase_name + \"/bn\")(x)\n\n\n\n    shortcut = Conv2D(filters3, (1, 1), strides=strides, use_bias=bias,\n\n                      name=conv1_proj_name)(input_tensor)\n\n    shortcut = BatchNormalization(axis=bn_axis, name=conv1_proj_name + \"/bn\")(\n\n        shortcut)\n\n\n\n    x = layers.add([x, shortcut])\n\n    x = Activation('relu')(x)\n\n    return x\n\n\n\n\n\ndef RESNET50(include_top=True, weights='vggface',\n\n             input_tensor=None, input_shape=None,\n\n             pooling=None,\n\n             classes=8631):\n\n    input_shape = _obtain_input_shape(input_shape,\n\n                                      default_size=224,\n\n                                      min_size=32,\n\n                                      data_format=K.image_data_format(),\n\n                                      require_flatten=include_top,\n\n                                      weights=weights)\n\n\n\n    if input_tensor is None:\n\n        img_input = Input(shape=input_shape)\n\n    else:\n\n        if not K.is_keras_tensor(input_tensor):\n\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n\n        else:\n\n            img_input = input_tensor\n\n    if K.image_data_format() == 'channels_last':\n\n        bn_axis = 3\n\n    else:\n\n        bn_axis = 1\n\n\n\n    x = Conv2D(\n\n        64, (7, 7), use_bias=False, strides=(2, 2), padding='same',\n\n        name='conv1/7x7_s2')(img_input)\n\n    x = BatchNormalization(axis=bn_axis, name='conv1/7x7_s2/bn')(x)\n\n    x = Activation('relu')(x)\n\n    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n\n\n\n    x = resnet_conv_block(x, 3, [64, 64, 256], stage=2, block=1, strides=(1, 1))\n\n    x = resnet_identity_block(x, 3, [64, 64, 256], stage=2, block=2)\n\n    x = resnet_identity_block(x, 3, [64, 64, 256], stage=2, block=3)\n\n\n\n    x = resnet_conv_block(x, 3, [128, 128, 512], stage=3, block=1)\n\n    x = resnet_identity_block(x, 3, [128, 128, 512], stage=3, block=2)\n\n    x = resnet_identity_block(x, 3, [128, 128, 512], stage=3, block=3)\n\n    x = resnet_identity_block(x, 3, [128, 128, 512], stage=3, block=4)\n\n\n\n    x = resnet_conv_block(x, 3, [256, 256, 1024], stage=4, block=1)\n\n    x = resnet_identity_block(x, 3, [256, 256, 1024], stage=4, block=2)\n\n    x = resnet_identity_block(x, 3, [256, 256, 1024], stage=4, block=3)\n\n    x = resnet_identity_block(x, 3, [256, 256, 1024], stage=4, block=4)\n\n    x = resnet_identity_block(x, 3, [256, 256, 1024], stage=4, block=5)\n\n    x = resnet_identity_block(x, 3, [256, 256, 1024], stage=4, block=6)\n\n\n\n    x = resnet_conv_block(x, 3, [512, 512, 2048], stage=5, block=1)\n\n    x = resnet_identity_block(x, 3, [512, 512, 2048], stage=5, block=2)\n\n    x = resnet_identity_block(x, 3, [512, 512, 2048], stage=5, block=3)\n\n\n\n    x = AveragePooling2D((7, 7), name='avg_pool')(x)\n\n\n\n    if include_top:\n\n        x = Flatten()(x)\n\n        x = Dense(classes, activation='softmax', name='classifier')(x)\n\n    else:\n\n        if pooling == 'avg':\n\n            x = GlobalAveragePooling2D()(x)\n\n        elif pooling == 'max':\n\n            x = GlobalMaxPooling2D()(x)\n\n\n\n    # Ensure that the model takes into account\n\n    # any potential predecessors of `input_tensor`.\n\n    if input_tensor is not None:\n\n        inputs = get_source_inputs(input_tensor)\n\n    else:\n\n        inputs = img_input\n\n    # Create model.\n\n    model = Model(inputs, x, name='vggface_resnet50')\n\n\n\n    # load weights\n\n    if weights == 'vggface':\n\n        if include_top:\n\n            weights_path = get_file('rcmalli_vggface_tf_resnet50.h5',\n\n                                    utils.RESNET50_WEIGHTS_PATH,\n\n                                    cache_subdir=utils.VGGFACE_DIR)\n\n        else:\n\n            weights_path = get_file('rcmalli_vggface_tf_notop_resnet50.h5',\n\n                                    utils.RESNET50_WEIGHTS_PATH_NO_TOP,\n\n                                    cache_subdir=utils.VGGFACE_DIR)\n\n        model.load_weights(weights_path)\n\n        if K.backend() == 'theano':\n\n            layer_utils.convert_all_kernels_in_model(model)\n\n            if include_top:\n\n                maxpool = model.get_layer(name='avg_pool')\n\n                shape = maxpool.output_shape[1:]\n\n                dense = model.get_layer(name='classifier')\n\n                layer_utils.convert_dense_weights_data_format(dense, shape,\n\n                                                              'channels_first')\n\n\n\n        if K.image_data_format() == 'channels_first' and K.backend() == 'tensorflow':\n\n            warnings.warn('You are using the TensorFlow backend, yet you '\n\n                          'are using the Theano '\n\n                          'image data format convention '\n\n                          '(`image_data_format=\"channels_first\"`). '\n\n                          'For best performance, set '\n\n                          '`image_data_format=\"channels_last\"` in '\n\n                          'your Keras config '\n\n                          'at ~/.keras/keras.json.')\n\n    elif weights is not None:\n\n        model.load_weights(weights)\n\n\n\n    return model\n\n\n\n\n\ndef senet_se_block(input_tensor, stage, block, compress_rate=16, bias=False):\n\n    conv1_down_name = 'conv' + str(stage) + \"_\" + str(\n\n        block) + \"_1x1_down\"\n\n    conv1_up_name = 'conv' + str(stage) + \"_\" + str(\n\n        block) + \"_1x1_up\"\n\n\n\n    num_channels = int(input_tensor.shape[-1])\n\n    bottle_neck = int(num_channels // compress_rate)\n\n\n\n    se = GlobalAveragePooling2D()(input_tensor)\n\n    se = Reshape((1, 1, num_channels))(se)\n\n    se = Conv2D(bottle_neck, (1, 1), use_bias=bias,\n\n                name=conv1_down_name)(se)\n\n    se = Activation('relu')(se)\n\n    se = Conv2D(num_channels, (1, 1), use_bias=bias,\n\n                name=conv1_up_name)(se)\n\n    se = Activation('sigmoid')(se)\n\n\n\n    x = input_tensor\n\n    x = multiply([x, se])\n\n    return x\n\n\n\n\n\ndef senet_conv_block(input_tensor, kernel_size, filters,\n\n                     stage, block, bias=False, strides=(2, 2)):\n\n    filters1, filters2, filters3 = filters\n\n    if K.image_data_format() == 'channels_last':\n\n        bn_axis = 3\n\n    else:\n\n        bn_axis = 1\n\n\n\n    bn_eps = 0.0001\n\n\n\n    conv1_reduce_name = 'conv' + str(stage) + \"_\" + str(block) + \"_1x1_reduce\"\n\n    conv1_increase_name = 'conv' + str(stage) + \"_\" + str(\n\n        block) + \"_1x1_increase\"\n\n    conv1_proj_name = 'conv' + str(stage) + \"_\" + str(block) + \"_1x1_proj\"\n\n    conv3_name = 'conv' + str(stage) + \"_\" + str(block) + \"_3x3\"\n\n\n\n    x = Conv2D(filters1, (1, 1), use_bias=bias, strides=strides,\n\n               name=conv1_reduce_name)(input_tensor)\n\n    x = BatchNormalization(axis=bn_axis, name=conv1_reduce_name + \"/bn\",epsilon=bn_eps)(x)\n\n    x = Activation('relu')(x)\n\n\n\n    x = Conv2D(filters2, kernel_size, padding='same', use_bias=bias,\n\n               name=conv3_name)(x)\n\n    x = BatchNormalization(axis=bn_axis, name=conv3_name + \"/bn\",epsilon=bn_eps)(x)\n\n    x = Activation('relu')(x)\n\n\n\n    x = Conv2D(filters3, (1, 1), name=conv1_increase_name, use_bias=bias)(x)\n\n    x = BatchNormalization(axis=bn_axis, name=conv1_increase_name + \"/bn\" ,epsilon=bn_eps)(x)\n\n\n\n    se = senet_se_block(x, stage=stage, block=block, bias=True)\n\n\n\n    shortcut = Conv2D(filters3, (1, 1), use_bias=bias, strides=strides,\n\n                      name=conv1_proj_name)(input_tensor)\n\n    shortcut = BatchNormalization(axis=bn_axis,\n\n                                  name=conv1_proj_name + \"/bn\",epsilon=bn_eps)(shortcut)\n\n\n\n    m = layers.add([se, shortcut])\n\n    m = Activation('relu')(m)\n\n    return m\n\n\n\n\n\ndef senet_identity_block(input_tensor, kernel_size,\n\n                         filters, stage, block, bias=False):\n\n    filters1, filters2, filters3 = filters\n\n    if K.image_data_format() == 'channels_last':\n\n        bn_axis = 3\n\n    else:\n\n        bn_axis = 1\n\n\n\n    bn_eps = 0.0001\n\n\n\n    conv1_reduce_name = 'conv' + str(stage) + \"_\" + str(block) + \"_1x1_reduce\"\n\n    conv1_increase_name = 'conv' + str(stage) + \"_\" + str(\n\n        block) + \"_1x1_increase\"\n\n    conv3_name = 'conv' + str(stage) + \"_\" + str(block) + \"_3x3\"\n\n\n\n    x = Conv2D(filters1, (1, 1), use_bias=bias,\n\n               name=conv1_reduce_name)(input_tensor)\n\n    x = BatchNormalization(axis=bn_axis, name=conv1_reduce_name + \"/bn\",epsilon=bn_eps)(x)\n\n    x = Activation('relu')(x)\n\n\n\n    x = Conv2D(filters2, kernel_size, padding='same', use_bias=bias,\n\n               name=conv3_name)(x)\n\n    x = BatchNormalization(axis=bn_axis, name=conv3_name + \"/bn\",epsilon=bn_eps)(x)\n\n    x = Activation('relu')(x)\n\n\n\n    x = Conv2D(filters3, (1, 1), name=conv1_increase_name, use_bias=bias)(x)\n\n    x = BatchNormalization(axis=bn_axis, name=conv1_increase_name + \"/bn\",epsilon=bn_eps)(x)\n\n\n\n    se = senet_se_block(x, stage=stage, block=block, bias=True)\n\n\n\n    m = layers.add([se, input_tensor])\n\n    m = Activation('relu')(m)\n\n\n\n    return m\n\n\n\n\n\ndef SENET50(include_top=True, weights='vggface',\n\n            input_tensor=None, input_shape=None,\n\n            pooling=None,\n\n            classes=8631):\n\n    input_shape = _obtain_input_shape(input_shape,\n\n                                      default_size=224,\n\n                                      min_size=197,\n\n                                      data_format=K.image_data_format(),\n\n                                      require_flatten=include_top,\n\n                                      weights=weights)\n\n\n\n    if input_tensor is None:\n\n        img_input = Input(shape=input_shape)\n\n    else:\n\n        if not K.is_keras_tensor(input_tensor):\n\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n\n        else:\n\n            img_input = input_tensor\n\n    if K.image_data_format() == 'channels_last':\n\n        bn_axis = 3\n\n    else:\n\n        bn_axis = 1\n\n\n\n    bn_eps = 0.0001\n\n\n\n    x = Conv2D(\n\n        64, (7, 7), use_bias=False, strides=(2, 2), padding='same',\n\n        name='conv1/7x7_s2')(img_input)\n\n    x = BatchNormalization(axis=bn_axis, name='conv1/7x7_s2/bn',epsilon=bn_eps)(x)\n\n    x = Activation('relu')(x)\n\n    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n\n\n\n    x = senet_conv_block(x, 3, [64, 64, 256], stage=2, block=1, strides=(1, 1))\n\n    x = senet_identity_block(x, 3, [64, 64, 256], stage=2, block=2)\n\n    x = senet_identity_block(x, 3, [64, 64, 256], stage=2, block=3)\n\n\n\n    x = senet_conv_block(x, 3, [128, 128, 512], stage=3, block=1)\n\n    x = senet_identity_block(x, 3, [128, 128, 512], stage=3, block=2)\n\n    x = senet_identity_block(x, 3, [128, 128, 512], stage=3, block=3)\n\n    x = senet_identity_block(x, 3, [128, 128, 512], stage=3, block=4)\n\n\n\n    x = senet_conv_block(x, 3, [256, 256, 1024], stage=4, block=1)\n\n    x = senet_identity_block(x, 3, [256, 256, 1024], stage=4, block=2)\n\n    x = senet_identity_block(x, 3, [256, 256, 1024], stage=4, block=3)\n\n    x = senet_identity_block(x, 3, [256, 256, 1024], stage=4, block=4)\n\n    x = senet_identity_block(x, 3, [256, 256, 1024], stage=4, block=5)\n\n    x = senet_identity_block(x, 3, [256, 256, 1024], stage=4, block=6)\n\n\n\n    x = senet_conv_block(x, 3, [512, 512, 2048], stage=5, block=1)\n\n    x = senet_identity_block(x, 3, [512, 512, 2048], stage=5, block=2)\n\n    x = senet_identity_block(x, 3, [512, 512, 2048], stage=5, block=3)\n\n\n\n    x = AveragePooling2D((7, 7), name='avg_pool')(x)\n\n\n\n    if include_top:\n\n        x = Flatten()(x)\n\n        x = Dense(classes, activation='softmax', name='classifier')(x)\n\n    else:\n\n        if pooling == 'avg':\n\n            x = GlobalAveragePooling2D()(x)\n\n        elif pooling == 'max':\n\n            x = GlobalMaxPooling2D()(x)\n\n\n\n    # Ensure that the model takes into account\n\n    # any potential predecessors of `input_tensor`.\n\n    if input_tensor is not None:\n\n        inputs = get_source_inputs(input_tensor)\n\n    else:\n\n        inputs = img_input\n\n    # Create model.\n\n    model = Model(inputs, x, name='vggface_senet50')\n\n\n\n    # load weights\n\n    if weights == 'vggface':\n\n        if include_top:\n\n            weights_path = get_file('rcmalli_vggface_tf_senet50.h5',\n\n                                    utils.SENET50_WEIGHTS_PATH,\n\n                                    cache_subdir=utils.VGGFACE_DIR)\n\n        else:\n\n            weights_path = get_file('rcmalli_vggface_tf_notop_senet50.h5',\n\n                                    utils.SENET50_WEIGHTS_PATH_NO_TOP,\n\n                                    cache_subdir=utils.VGGFACE_DIR)\n\n        model.load_weights(weights_path)\n\n        if K.backend() == 'theano':\n\n            layer_utils.convert_all_kernels_in_model(model)\n\n            if include_top:\n\n                maxpool = model.get_layer(name='avg_pool')\n\n                shape = maxpool.output_shape[1:]\n\n                dense = model.get_layer(name='classifier')\n\n                layer_utils.convert_dense_weights_data_format(dense, shape,\n\n                                                              'channels_first')\n\n\n\n        if K.image_data_format() == 'channels_first' and K.backend() == 'tensorflow':\n\n            warnings.warn('You are using the TensorFlow backend, yet you '\n\n                          'are using the Theano '\n\n                          'image data format convention '\n\n                          '(`image_data_format=\"channels_first\"`). '\n\n                          'For best performance, set '\n\n                          '`image_data_format=\"channels_last\"` in '\n\n                          'your Keras config '\n\n                          'at ~/.keras/keras.json.')\n\n    elif weights is not None:\n\n        model.load_weights(weights)\n\n\n\n    return model\n\n"
     ]
    }
   ],
   "source": [
    "# with open('/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/keras_vggface/models.py', 'r') as file:\n",
    "#     for line in file:\n",
    "#         print(line)\n",
    "# #     content = file.read()\n",
    "# # content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdf60858-120a-43bd-a537-d8826cb1f22a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import errno\n",
    "import imageio\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "\n",
    "         \n",
    "      # from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "        # from tensorflow.python.keras import models\n",
    "from keras import models\n",
    "        # from tensorflow.keras import models\n",
    "        # from tensorflow.python.keras import layers\n",
    "from keras import layers\n",
    "        # from tensorflow.keras import layers\n",
    "        # from tensorflow.keras import layers\n",
    "# from tensorflow.python.keras import Model\n",
    "from keras import Model\n",
    "      # from tensorflow.keras import Model\n",
    "# from keras.optimizers import SGD\n",
    "        # from tensorflow.keras.optimizers import SGD\n",
    "# from keras.optimizers.legacy import SGD\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras_vggface.vggface import VGGFace\n",
    "      #from keras.utils import get_source_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82e5f8e8-56d0-43a2-b132-d847550b81e4",
     "showTitle": true,
     "title": "data_download"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# initialization\n",
    "image_height = 48\n",
    "image_width = 48\n",
    "window_size = 24\n",
    "window_step = 6\n",
    "SAVE_IMAGES = True\n",
    "SELECTED_LABELS = [0,1,2,3,4,5,6]\n",
    "IMAGES_PER_LABEL = 500\n",
    "OUTPUT_FOLDER_NAME = \"/FileStore/demo_store\"\n",
    "\n",
    "def data_download():\n",
    "    print( \"preparing\")\n",
    "    try:\n",
    "        os.makedirs(OUTPUT_FOLDER_NAME)\n",
    "    except OSError as e:\n",
    "        if e.errno == errno.EEXIST and os.path.isdir(OUTPUT_FOLDER_NAME):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "            \n",
    "    \n",
    "    print( \"importing csv file\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    data_set_path = \"/FileStore/demo_data/fer2013.csv\"\n",
    "    data = spark.read.csv(data_set_path,header=True,inferSchema=True)\n",
    "    # display(data)\n",
    "    data = data.toPandas()\n",
    "\n",
    "    # data = pd.read_csv('../datasets/raw/fer2013.csv') # data from kaggle stored here as csv file.\n",
    "    print(data.head(2))\n",
    "\n",
    "    for category in data['Usage'].unique():\n",
    "        # print(category)\n",
    "        # time.sleep(3)\n",
    "        print( \"converting set: \" + category + \"...\")\n",
    "        # create folder\n",
    "        if not os.path.exists(category):\n",
    "            try:\n",
    "                os.makedirs(OUTPUT_FOLDER_NAME + '/' + category)\n",
    "            except OSError as e:\n",
    "                if e.errno == errno.EEXIST and os.path.isdir(OUTPUT_FOLDER_NAME):\n",
    "                   pass\n",
    "                else:\n",
    "                    raise\n",
    "        \n",
    "        # get samples and labels of the actual category\n",
    "        category_data = data[data['Usage'] == category]\n",
    "        samples = category_data['pixels'].values\n",
    "        labels = category_data['emotion'].values\n",
    "        \n",
    "        # print(\"\\ncategory:\",category_data,\"\\nsamples:\",samples[0],\"\\nlabels:\",labels)\n",
    "        # time.sleep(3)\n",
    "        # get images and extract features\n",
    "        images = []\n",
    "   \n",
    "        print(\"Length of samples,labels\\n:::::\")\n",
    "        print(len(samples),len(labels))\n",
    "        print(\"\\nSelected labels:\",SELECTED_LABELS)\n",
    "        # print(type(labels),type(samples))\n",
    "        # time.sleep(2)\n",
    "        for i in range(len(samples)):\n",
    "            # print(\"inside for loop:\",i)\n",
    "            try:\n",
    "                if labels[i] in SELECTED_LABELS: \n",
    "                    if i%1000 == 0:\n",
    "                        print(i,\"inside if class:\")\n",
    "                    # image = np.fromstring(samples[i], dtype=int, sep=\" \").reshape((image_height, image_width))\n",
    "                    image = np.fromstring(samples[i], dtype=np.uint8, sep=\" \").reshape((image_height, image_width))\n",
    "                    images.append(image)\n",
    "                    # print(image)\n",
    "                    imageio.imwrite(OUTPUT_FOLDER_NAME + '/' + category + '/' + str(i) + '.jpg', image)\n",
    "                    # break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print( \"error in image: \" + str(i) + \" - \" + str(e))\n",
    "                break\n",
    "        print(\"image count\",len(images))        \n",
    "        np.save(OUTPUT_FOLDER_NAME + '/' + category + '/images.npy', images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66aae9bb-2875-4ce2-8b97-371166d14c02",
     "showTitle": true,
     "title": "data_generator"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def data_generator(train_dir, valid_dir, test_dir, train_label, valid_label, test_label):\n",
    "    # Add our data-augmentation parameters to ImageDataGenerator\n",
    "    train_datagen = ImageDataGenerator(#rescale = 1./255.,\n",
    "                                       rotation_range = 40,\n",
    "                                       width_shift_range = 0.2,\n",
    "                                       height_shift_range = 0.2,\n",
    "                                       shear_range = 0.2,\n",
    "                                       zoom_range = 0.2,\n",
    "                                       horizontal_flip = True)\n",
    "    \n",
    "    # Note that the validation data should not be augmented!\n",
    "    valid_datagen = ImageDataGenerator()#rescale = 1./255. )\n",
    "    test_datagen = ImageDataGenerator()#rescale = 1./255. )\n",
    "    \n",
    "    # Flow training images in batches of 5 using train_datagen generator\n",
    "    train_generator=train_datagen.flow_from_dataframe(dataframe=train_label,\n",
    "                                                     directory=train_dir,\n",
    "                                                     x_col=\"id\",\n",
    "                                                     y_col=\"emotion\",\n",
    "                                                     target_size=(96,96),\n",
    "                                                     batch_size=32,\n",
    "                                                     seed=42,\n",
    "                                                     shuffle=True,\n",
    "                                                     class_mode=\"categorical\",\n",
    "                                                     color_mode='rgb')\n",
    "    # train_generator=itertools.islice(itertools.cycle(train_generator),896)#By using itertools.cycle() and itertools.islice(), you can achieve a similar effect to the .repeat()\n",
    "    # train_generator=train_generator.repeat() // I deleted steps_per_epoch\n",
    "\n",
    "    \n",
    "    # # Flow validation images in batches of 5 using test_datagen generator\n",
    "    valid_generator=valid_datagen.flow_from_dataframe(dataframe=valid_label,\n",
    "                                                      directory=valid_dir,\n",
    "                                                      x_col=\"id\",\n",
    "                                                      y_col=\"emotion\",\n",
    "                                                      target_size=(96,96),\n",
    "                                                      batch_size=32,\n",
    "                                                      seed=42,\n",
    "                                                      shuffle=True,\n",
    "                                                      class_mode=\"categorical\",\n",
    "                                                      color_mode='rgb')\n",
    "    \n",
    "    # valid_generator=itertools.islice(itertools.cycle(valid_generator),896)#By using itertools.cycle() and itertools.islice(), you can achieve a similar effect to the .repeat()\n",
    "    # valid_generator=valid_generator.repeat() I deleted steps_per_valid\n",
    "    # Flow validation images in batches of 5 using test_datagen generator\n",
    "    test_generator=test_datagen.flow_from_dataframe(dataframe=test_label,\n",
    "                                                     directory=test_dir,\n",
    "                                                     x_col=\"id\",\n",
    "                                                     y_col=None,\n",
    "                                                     target_size=(96,96),\n",
    "                                                     batch_size=32,\n",
    "                                                     seed=42,\n",
    "                                                     shuffle=False,\n",
    "                                                     class_mode=None,\n",
    "                                                     color_mode='rgb')\n",
    "\n",
    "    return train_generator, valid_generator, test_generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b091b844-62e3-48a4-951e-a5df1ec2094b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 30 May 2023\n",
    "\n",
    "Author: Bibin Kunjumon\n",
    "\"\"\"\n",
    "\n",
    "def append_ext(fn):\n",
    "    return fn+\".jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a86e81c5-8d79-4ca9-88fe-4b06fcaf4851",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 30 May 2023\n",
    "File: label_and_dir.py\n",
    "Author: Bibin Kunjumon\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def label_and_dir():\n",
    "    # Load the dataset\n",
    "    data_set_path = \"/FileStore/demo_store/traininglabel.csv\"\n",
    "    train_label_spark = spark.read.csv(data_set_path,header=True,inferSchema=True)\n",
    "    train_label = train_label_spark.toPandas().astype(str)\n",
    "\n",
    "    data_set_path = \"/FileStore/demo_store/publictestlabel.csv\"\n",
    "    valid_label_spark = spark.read.csv(data_set_path,header=True,inferSchema=True)\n",
    "    valid_label = valid_label_spark.toPandas().astype(str)\n",
    "\n",
    "    data_set_path = \"/FileStore/demo_store/privatetestlabel.csv\"\n",
    "    test_label_spark = spark.read.csv(data_set_path,header=True,inferSchema=True)\n",
    "    test_label = test_label_spark.toPandas().astype(str)\n",
    "\n",
    "\n",
    "    # train_label = pd.read_csv('/FileStore/demo_store/traininglabel.csv',dtype=str)\n",
    "    # valid_label = pd.read_csv('/FileStore/demo_store/publictestlabel.csv',dtype=str)\n",
    "    # test_label = pd.read_csv('/FileStore/demo_store/privatetestlabel.csv',dtype=str)\n",
    "    \n",
    "    train_label[\"id\"]=train_label[\"id\"].apply(append_ext)\n",
    "    valid_label[\"id\"]=valid_label[\"id\"].apply(append_ext)\n",
    "    test_label[\"id\"]=test_label[\"id\"].apply(append_ext)\n",
    "    \n",
    "    # Define our example directories and files\n",
    "    train_dir = '/FileStore/demo_store/Training'\n",
    "    valid_dir = '/FileStore/demo_store/PublicTest'\n",
    "    test_dir = '/FileStore/demo_store/PrivateTest'\n",
    "    return train_dir, valid_dir, test_dir, train_label, valid_label, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a515cd24-27c8-45b4-8076-b34c92b4aa11",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing\nimporting csv file\n   emotion                                             pixels     Usage\n0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\nconverting set: Training...\nLength of samples,labels\n:::::\n28709 28709\n\nSelected labels: [0, 1, 2, 3, 4, 5, 6]\n0 inside if class:\n1000 inside if class:\n2000 inside if class:\n3000 inside if class:\n4000 inside if class:\n5000 inside if class:\n6000 inside if class:\n7000 inside if class:\n8000 inside if class:\n9000 inside if class:\n10000 inside if class:\n11000 inside if class:\n12000 inside if class:\n13000 inside if class:\n14000 inside if class:\n15000 inside if class:\n16000 inside if class:\n17000 inside if class:\n18000 inside if class:\n19000 inside if class:\n20000 inside if class:\n21000 inside if class:\n22000 inside if class:\n23000 inside if class:\n24000 inside if class:\n25000 inside if class:\n26000 inside if class:\n27000 inside if class:\n28000 inside if class:\nimage count 28709\nconverting set: PublicTest...\nLength of samples,labels\n:::::\n3589 3589\n\nSelected labels: [0, 1, 2, 3, 4, 5, 6]\n0 inside if class:\n1000 inside if class:\n2000 inside if class:\n3000 inside if class:\nimage count 3589\nconverting set: PrivateTest...\nLength of samples,labels\n:::::\n3589 3589\n\nSelected labels: [0, 1, 2, 3, 4, 5, 6]\n0 inside if class:\n1000 inside if class:\n2000 inside if class:\n3000 inside if class:\nimage count 3589\nFound 28709 validated image filenames belonging to 7 classes.\nFound 3589 validated image filenames belonging to 7 classes.\nFound 3589 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#downloading data from Kaggle\n",
    "data_download()\n",
    "\n",
    "#importing data\n",
    "train_dir, valid_dir, test_dir, train_label, valid_label, test_label = label_and_dir()\n",
    "train_generator, valid_generator, test_generator = data_generator(train_dir, valid_dir, test_dir, train_label, valid_label, test_label)\n",
    "\n",
    "#custom parameters\n",
    "nb_class = 7\n",
    "hidden_dim = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29ffd169-1490-4edb-8013-bac5c032ec4a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 3, 3, 512)\n"
     ]
    }
   ],
   "source": [
    "vgg_model = VGGFace(include_top=False, input_shape=(96, 96, 3))\n",
    "last_layer = vgg_model.get_layer('pool5').output\n",
    "print(last_layer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3296c855-5114-4de5-baca-a0677bd6ad24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 4608) dtype=float32 (created by layer 'flatten_2')>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers.Flatten()(last_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf4fd3bc-94ef-4bf9-8de4-00c8bb3e3f11",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Printing the model summary\n\nModel: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 96, 96, 3)]       0         \n                                                                 \n conv1_1 (Conv2D)            (None, 96, 96, 64)        1792      \n                                                                 \n conv1_2 (Conv2D)            (None, 96, 96, 64)        36928     \n                                                                 \n pool1 (MaxPooling2D)        (None, 48, 48, 64)        0         \n                                                                 \n conv2_1 (Conv2D)            (None, 48, 48, 128)       73856     \n                                                                 \n conv2_2 (Conv2D)            (None, 48, 48, 128)       147584    \n                                                                 \n pool2 (MaxPooling2D)        (None, 24, 24, 128)       0         \n                                                                 \n conv3_1 (Conv2D)            (None, 24, 24, 256)       295168    \n                                                                 \n conv3_2 (Conv2D)            (None, 24, 24, 256)       590080    \n                                                                 \n conv3_3 (Conv2D)            (None, 24, 24, 256)       590080    \n                                                                 \n pool3 (MaxPooling2D)        (None, 12, 12, 256)       0         \n                                                                 \n conv4_1 (Conv2D)            (None, 12, 12, 512)       1180160   \n                                                                 \n conv4_2 (Conv2D)            (None, 12, 12, 512)       2359808   \n                                                                 \n conv4_3 (Conv2D)            (None, 12, 12, 512)       2359808   \n                                                                 \n pool4 (MaxPooling2D)        (None, 6, 6, 512)         0         \n                                                                 \n conv5_1 (Conv2D)            (None, 6, 6, 512)         2359808   \n                                                                 \n conv5_2 (Conv2D)            (None, 6, 6, 512)         2359808   \n                                                                 \n conv5_3 (Conv2D)            (None, 6, 6, 512)         2359808   \n                                                                 \n pool5 (MaxPooling2D)        (None, 3, 3, 512)         0         \n                                                                 \n flatten_3 (Flatten)         (None, 4608)              0         \n                                                                 \n fc7 (Dense)                 (None, 1024)              4719616   \n                                                                 \n fc8 (Dense)                 (None, 7)                 7175      \n                                                                 \n=================================================================\nTotal params: 19,441,479\nTrainable params: 19,441,479\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x = layers.Flatten()(last_layer)\n",
    "x = layers.Dense(hidden_dim, activation='relu', name='fc7')(x)\n",
    "x = layers.Dense(nb_class, activation='softmax', name='fc8')(x)\n",
    "custom_vgg_model = Model(vgg_model.input, x)\n",
    "\n",
    "print(\"---Printing the model summary\\n\")\n",
    "#Printing the model summary\n",
    "custom_vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7138d47-772d-4ef4-94c3-80eee8eaa519",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg_model compiled\n"
     ]
    }
   ],
   "source": [
    "# Training the model with fer2013 data.\n",
    "custom_vgg_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(learning_rate=0.0001),\n",
    "              metrics=['accuracy'])\n",
    "print(\"vgg_model compiled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bd2b9e9-f9bf-438b-98a7-c68ac2cf3e7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Creating a callback that saves a model when the validation loss decreases from the previous epoch.\n",
    "filepath=\"/FileStore/demo_store/epoch_store/weights-improvement-{epoch:02d}-{val_loss:.3f}\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor ='val_loss', verbose=1, save_best_only=False, mode='auto', save_weights_only = False)\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "# dbutils.fs.put(\"/FileStore/demo_store/trained_models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3392858-6713-47f7-8cd6-412be16b906e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import urllib3\n",
    "\n",
    "http = urllib3.PoolManager(maxsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bca8e94-201c-4036-ba2e-a1e1142b1aa8",
     "showTitle": true,
     "title": "Suppress warnings"
    }
   },
   "outputs": [],
   "source": [
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03da2dbb-2630-4a69-a8ab-f14ef23d507c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Starting the model.fit: each Epoch takes time \")\n",
    "history = custom_vgg_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=valid_generator,validation_split=0.1,\n",
    "    # steps_per_epoch=897, #TensorFlow will automatically determine it based on the number of available batches in the training generator.\n",
    "    epochs=30,\n",
    "    # validation_steps=897,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e092f9b0-1275-4ed0-92ab-336fa2ad6f2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 13). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /FileStore/demo_store/trained_models/emotion_trained_june/assets\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /FileStore/demo_store/trained_models/emotion_trained_june/assets\n"
     ]
    }
   ],
   "source": [
    "custom_vgg_model.save(\"/FileStore/demo_store/trained_models/emotion_trained_june\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8be8707-9035-4d24-9dd4-fc9e9e0ac56d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %rm /FileStore/demo_store/trained_models/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da3f53df-7452-4d34-8b8d-e1ac8cabea90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d83bf523-b5fc-4134-9751-e880f3c79629",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "my_model = load_model(\"/FileStore/demo_store/trained_models/emotion_trained_june\")\n",
    "# my_model = load_model(\"/dbfs/FileStore/demo_store/built_model/trained_vggface.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43301338-5415-41b7-9154-e1aa70d16028",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 96, 96, 3)]       0         \n                                                                 \n conv1_1 (Conv2D)            (None, 96, 96, 64)        1792      \n                                                                 \n conv1_2 (Conv2D)            (None, 96, 96, 64)        36928     \n                                                                 \n pool1 (MaxPooling2D)        (None, 48, 48, 64)        0         \n                                                                 \n conv2_1 (Conv2D)            (None, 48, 48, 128)       73856     \n                                                                 \n conv2_2 (Conv2D)            (None, 48, 48, 128)       147584    \n                                                                 \n pool2 (MaxPooling2D)        (None, 24, 24, 128)       0         \n                                                                 \n conv3_1 (Conv2D)            (None, 24, 24, 256)       295168    \n                                                                 \n conv3_2 (Conv2D)            (None, 24, 24, 256)       590080    \n                                                                 \n conv3_3 (Conv2D)            (None, 24, 24, 256)       590080    \n                                                                 \n pool3 (MaxPooling2D)        (None, 12, 12, 256)       0         \n                                                                 \n conv4_1 (Conv2D)            (None, 12, 12, 512)       1180160   \n                                                                 \n conv4_2 (Conv2D)            (None, 12, 12, 512)       2359808   \n                                                                 \n conv4_3 (Conv2D)            (None, 12, 12, 512)       2359808   \n                                                                 \n pool4 (MaxPooling2D)        (None, 6, 6, 512)         0         \n                                                                 \n conv5_1 (Conv2D)            (None, 6, 6, 512)         2359808   \n                                                                 \n conv5_2 (Conv2D)            (None, 6, 6, 512)         2359808   \n                                                                 \n conv5_3 (Conv2D)            (None, 6, 6, 512)         2359808   \n                                                                 \n pool5 (MaxPooling2D)        (None, 3, 3, 512)         0         \n                                                                 \n flatten_3 (Flatten)         (None, 4608)              0         \n                                                                 \n fc7 (Dense)                 (None, 1024)              4719616   \n                                                                 \n fc8 (Dense)                 (None, 7)                 7175      \n                                                                 \n=================================================================\nTotal params: 19,441,479\nTrainable params: 19,441,479\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model.summary()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "my_project_4 (1)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
